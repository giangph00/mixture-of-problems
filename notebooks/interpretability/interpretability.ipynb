{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91c89761",
   "metadata": {
    "time_run": "2026-02-08T16:08:57.333744+00:00"
   },
   "source": [
    "# Fruit Quality Inspection and Generation\n",
    "\n",
    "Automated food inspection requires not just accurate models, but *interpretable* ones. Before deploying a system to sort fresh produce from spoiled, we need to understand why it makes decisions. Does the model classify a fruit as \"Rotten\" because of visible mold, or is it picking up on background artifacts?\n",
    "\n",
    "This notebook explores the internals of a deep learning model trained to inspect fruit. We'll build three visualization tools to examine its internal features, explain its decisions, and experiment with diffusion to generate some training data. A quick glance into the program:\n",
    "\n",
    "- **Visualizing Feature Hierarchy**: Observe how a CNN transforms pixels into abstract features\n",
    "- **Saliency Maps**: Identify which pixels most influence predictions\n",
    "- **Grad-CAM**: Generate heatmaps highlighting decision-driving regions\n",
    "- **Generating Synthetic Data**: Use Stable Diffusion to synthesize rare defect images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f570ac83",
   "metadata": {
    "time_run": "2026-02-08T15:53:45.422339+00:00"
   },
   "source": [
    "## Table of Contents\n",
    "- [Imports](#0)\n",
    "- [1 - Setting the Stage: Data and Model](#1)\n",
    "    - [1.1 - The Fruit Dataset](#1-1)\n",
    "    - [1.2 - Loading the Pre-trained Inspector](#1-2)\n",
    "    - [1.3 - Making a Prediction](#1-3)\n",
    "- [2 - Visualizing Internal Representations](#2)\n",
    "    - [2.1 - Hooking into the Hierarchy](#2-1)\n",
    "    - [2.2 - Capturing the Hierarchy](#2-2)\n",
    "    - [2.3 - Processing Feature Maps](#2-3)\n",
    "- [3 - Pixel Level Scrutiny: Saliency Maps](#3)\n",
    "- [4 - Regional Attention: Class Activation Maps](#4)\n",
    "- [5 - Comparison of Interpretability Techniques](#5)\n",
    "- [6 - (Optional) Generative AI for Synthetic Data](#6)\n",
    "    - [6.1 - Setting up Stable Diffusion](#6-1)\n",
    "    - [6.2 - Generating Synthetic Data](#6-2)\n",
    "    - [6.3 - Peeking into the Diffusion Process](#6-3)\n",
    "- [7 - Conclusion](#7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd5b0c2",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6cc54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769229a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "from diffusers import StableDiffusionPipeline\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "import helper_utils\n",
    "import unittests\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf815bc",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Setting the Stage: Data and Model\n",
    "\n",
    "Before auditing an AI's decision-making process, we need to establish the environment: the data representing the real-world problem and the pre-trained model that performs the inspection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1f3d1b",
   "metadata": {},
   "source": [
    "<a name='1-1'></a>\n",
    "### 1.1 - The Fruit Dataset\n",
    "\n",
    "We work with a curated subset of the [Fruit and Vegetable Disease (Healthy vs Rotten)](https://www.kaggle.com/datasets/muhammad0subhan/fruit-and-vegetable-disease-healthy-vs-rotten) dataset, focusing on three fruit categories: **Apples**, **Mangoes**, and **Tomatoes**, each with **Healthy** and **Rotten** conditions.\n",
    "\n",
    "- **Total images**: 60 (10 per category for each fruit type)\n",
    "- **Classes**: 2 (Fresh = 0, Rotten = 1)\n",
    "- **Structure**: `./fruits_subset/{Fruit}_{Condition}/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629caf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"./fruits_subset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203a3340",
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_utils.plot_samples_from_dataset(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e818303",
   "metadata": {},
   "source": [
    "<a name='1-2'></a>\n",
    "### 1.2 - Loading the Pre-trained Inspector\n",
    "\n",
    "We use a **ResNet-50** architecture adapted for binary classification (fresh vs rotten). The model comes pre-trained on this task, allowing us to focus directly on interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ef60c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fruits_model = helper_utils.load_model(\"./models/fruits_quality_model.pth\", device)\n",
    "fruits_model = fruits_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75eb701",
   "metadata": {},
   "source": [
    "The architecture summary shows how the network is structured—early convolutional layers capture low-level details, while deeper sequential blocks combine features into abstract patterns. Understanding this structure helps us choose which layers to probe for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a7e1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_utils.display_model_architecture(fruits_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0406ac8",
   "metadata": {},
   "source": [
    "<a name='1-3'></a>\n",
    "### 1.3 - Making a Prediction\n",
    "\n",
    "Let's see the model in action. The interactive tool below feeds images into the model and displays predictions in real time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8247bdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_utils.predict_fruit_quality(fruits_model, dataset_path, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e12e20f",
   "metadata": {},
   "source": [
    "The model gives answers, but no explanations. How did it know? Did it recognize skin texture, spot a defect, or just guess based on color? Right now it's a \"black box\"—in the next sections, we'll build tools to look inside."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc8c6a4",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Visualizing Internal Representations\n",
    "\n",
    "A CNN doesn't see an \"apple\" all at once—it builds understanding hierarchically. Early layers detect edges and textures; deeper layers combine these into complex patterns like stems, bruises, or mold patches. Here we'll observe this transformation in real time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9a8e03",
   "metadata": {},
   "source": [
    "<a name='2-1'></a>\n",
    "### 2.1 - Hooking into the Hierarchy\n",
    "\n",
    "PyTorch discards intermediate feature maps after the forward pass to save memory. To peek inside, we use **hooks**—functions registered to specific layers that intercept and save outputs as data flows through.\n",
    "\n",
    "The `grab` helper creates these hooks. It returns a closure that, when attached to a layer, saves that layer's output (detached from the computation graph) into a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50147678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab(activations, name):\n",
    "    \"\"\"\n",
    "    Creates a forward hook function to capture and store the output of a specific layer.\n",
    "\n",
    "    Arguments:\n",
    "        activations: A dictionary where the captured layer output will be stored.\n",
    "        name: The key under which the output tensor will be saved in the dictionary.\n",
    "\n",
    "    Returns:\n",
    "        _hook: The closure function to be registered as a hook.\n",
    "    \"\"\"\n",
    "    def _hook(_, __, out): \n",
    "        activations[name] = out.detach()\n",
    "    return _hook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869d2acd",
   "metadata": {},
   "source": [
    "<a name='2-2'></a>\n",
    "### 2.2 - Capturing the Hierarchy\n",
    "\n",
    "This function captures feature maps from five key points in the ResNet: `conv1` and the first convolution of each residual layer (`layer1` through `layer4`). This gives us a \"fingerprint\" of how the image is represented at different depths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ec58c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_feature_hierarchy(img, model):\n",
    "    \"\"\"\n",
    "    Visualizes the feature hierarchy of a CNN by capturing feature maps \n",
    "    from specific layers during a forward pass.\n",
    "\n",
    "    Arguments:\n",
    "        img: The input tensor (image) to process.\n",
    "        model: The pretrained neural network module to use for feature extraction.\n",
    "\n",
    "    Returns:\n",
    "        activations: A dictionary mapping layer names to their captured \n",
    "                     feature-map tensors.\n",
    "    \"\"\"\n",
    "    activations = {}\n",
    "    \n",
    "    layers = { \n",
    "        \"conv1\": model.conv1,\n",
    "        \"layer1\": model.layer1[0].conv1,\n",
    "        \"layer2\": model.layer2[0].conv1,\n",
    "        \"layer3\": model.layer3[0].conv1,\n",
    "        \"layer4\": model.layer4[0].conv1\n",
    "    } \n",
    "\n",
    "    hooks = []\n",
    "    for name, layer in layers.items():\n",
    "        hook_function = grab(activations, name)\n",
    "        hook_handle = layer.register_forward_hook(hook_function)\n",
    "        hooks.append(hook_handle)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _ = model(img) \n",
    "\n",
    "    for h in hooks:  \n",
    "        h.remove() \n",
    "\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b932af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"./fruits_subset/Apple_Healthy/FreshApple_3.jpg\"\n",
    "img = helper_utils.preprocess_image(image_path, device)\n",
    "\n",
    "activations = cnn_feature_hierarchy(img=img, model=fruits_model)\n",
    "\n",
    "print(\"Activations Keys and Shapes:\\n\")\n",
    "for name, tensor in activations.items():\n",
    "    print(f\"{name}:\\t{tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea71027",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"./fruits_subset/Apple_Healthy/FreshApple_3.jpg\"\n",
    "img = helper_utils.preprocess_image(image_path, device)\n",
    "activations = cnn_feature_hierarchy(img=img, model=fruits_model)\n",
    "helper_utils.display_feature_hierarchy(activations, img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298b1008",
   "metadata": {},
   "source": [
    "<a name='2-3'></a>\n",
    "### 2.3 - Processing Feature Maps\n",
    "\n",
    "Raw feature maps are hard to interpret—early layers have high resolution but few channels, while deeper layers have low resolution but hundreds of channels. This function creates a standardized \"visual strip\" by selecting the most active channel at each depth and resizing it to match the original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c215ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_map_strip(img, model):\n",
    "    \"\"\"\n",
    "    Processes an image through a model to extract, select, and upsample \n",
    "    representative feature maps from specific layers.\n",
    "\n",
    "    Arguments:\n",
    "        img: The input image tensor.\n",
    "        model: The pretrained neural network module used for feature extraction.\n",
    "\n",
    "    Returns:\n",
    "        upsampled: A list of tensors, each representing the most active \n",
    "                   channel from a specific layer, resized to 224x224 and \n",
    "                   normalized to [0, 1].\n",
    "    \"\"\"\n",
    "    feats = cnn_feature_hierarchy(img, model)\n",
    "    upsampled = [] \n",
    "\n",
    "    for name in [\"conv1\", \"layer1\", \"layer2\", \"layer3\", \"layer4\"]:\n",
    "        fm = feats[name]\n",
    "        avg_activation = torch.mean(fm, dim=(2, 3))\n",
    "        idx = torch.argmax(avg_activation)\n",
    "        sel = fm[:, idx:idx+1] \n",
    "        sel = F.interpolate(sel, size=(224, 224), mode=\"bilinear\", align_corners=False)\n",
    "        sel = (sel - sel.min()) / (sel.max() - sel.min() + 1e-8)\n",
    "        upsampled.append(sel)\n",
    "\n",
    "    return upsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d044f7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"./fruits_subset/Tomato_Rotten/rottenTomato_8.jpg\"\n",
    "img = helper_utils.preprocess_image(image_path, device)\n",
    "\n",
    "upsampled = feature_map_strip(img=img, model=fruits_model)\n",
    "\n",
    "print(\"Shape of the upsampled feature maps:\\n\")\n",
    "for i, name in enumerate([\"conv1\", \"layer1\", \"layer2\", \"layer3\", \"layer4\"]):\n",
    "    print(f\"{name}:  {upsampled[i].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af219688",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"./fruits_subset/Tomato_Rotten/rottenTomato_8.jpg\"\n",
    "img = helper_utils.preprocess_image(image_path, device)\n",
    "upsampled = feature_map_strip(img=img, model=fruits_model)\n",
    "helper_utils.visual_strip(upsampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c390485",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3 - Pixel Level Scrutiny: Saliency Maps\n",
    "\n",
    "Feature maps show *what* patterns the network detects, but not *which* ones matter for the decision. **Saliency Maps** answer this by computing the gradient of the prediction with respect to input pixels—essentially asking: \"If I slightly change this pixel, how much does your confidence change?\"\n",
    "\n",
    "This provides pixel-level sensitivity, helping verify that attention focuses on the fruit rather than background clutter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a5445d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saliency_map(model, image_tensor, class_idx):\n",
    "    \"\"\"\n",
    "    Generate a saliency map for a single image and class.\n",
    "\n",
    "    Arguments:\n",
    "        model: A trained CNN model instance; should be in evaluation mode.\n",
    "        image_tensor: Input image tensor with shape (1, 3, H, W).\n",
    "        class_idx: The integer index of the specific target class logit to explain.\n",
    "\n",
    "    Returns:\n",
    "        heatmap: A 2-D saliency heat-map normalised to [0, 1] with shape (H, W).\n",
    "    \"\"\" \n",
    "    image_tensor = image_tensor.clone()\n",
    "    image_tensor = image_tensor.detach()\n",
    "    image_tensor.requires_grad_()\n",
    "\n",
    "    output = model(image_tensor)\n",
    "    target_logit = output[0, class_idx]\n",
    "\n",
    "    model.zero_grad()\n",
    "    target_logit.backward()\n",
    "\n",
    "    grads = torch.abs(image_tensor.grad.data[0]).sum(dim=0)\n",
    "    grads -= grads.min()\n",
    "    grads /= (grads.max() - grads.min() + 1e-8)\n",
    "\n",
    "    heatmap = grads.detach()\n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050fd3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"./fruits_subset/Apple_Rotten/rottenApple_7.jpg\"\n",
    "img = helper_utils.preprocess_image(image_path, device)\n",
    "class_idx = 1  # 0 = fresh, 1 = rotten\n",
    "\n",
    "heatmap = saliency_map(model=fruits_model, image_tensor=img, class_idx=class_idx)\n",
    "\n",
    "print(f\"Shape: {heatmap.shape}\")\n",
    "print(f\"Range: min = {heatmap.min()}, max = {heatmap.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56679e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"./fruits_subset/Apple_Rotten/rottenApple_7.jpg\"\n",
    "img = helper_utils.preprocess_image(image_path, device)\n",
    "class_idx = 1\n",
    "\n",
    "heatmap = saliency_map(model=fruits_model, image_tensor=img, class_idx=class_idx)\n",
    "helper_utils.display_saliency(image_tensor=img, heatmap=heatmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa2f1cb",
   "metadata": {},
   "source": [
    "**Interpreting Saliency Maps:**\n",
    "- Bright regions indicate pixels that strongly influence the prediction\n",
    "- A well-trained model should highlight defects (brown spots, mold) rather than background\n",
    "- Saliency maps can be noisy—focus on coherent clusters rather than individual pixels\n",
    "- Sharp edges often appear salient simply due to high-frequency changes, not semantic importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b286b032",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "## 4 - Regional Attention: Class Activation Maps\n",
    "\n",
    "Saliency maps are powerful but noisy. Sometimes we want a broader answer: not \"which pixel?\" but \"which **region**?\"\n",
    "\n",
    "**Class Activation Maps (CAM)** combine the final convolutional layer's feature maps with classification weights to produce smooth heatmaps highlighting entire objects or regions the model focuses on.\n",
    "\n",
    "Since our ResNet-50 uses Global Average Pooling → FC, we can compute CAM directly by mapping FC weights back onto feature maps—no backpropagation needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596f9c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplified_cam(model, image_tensor, class_idx):\n",
    "    \"\"\"\n",
    "    Generates a simplified Class Activation Map (CAM) for a specific image and class.\n",
    "\n",
    "    Arguments:\n",
    "        model: A trained ResNet-style neural network module.\n",
    "        image_tensor: The input image tensor (1, 3, H, W), normalized for the model.\n",
    "        class_idx: The integer index of the target class to explain.\n",
    "\n",
    "    Returns:\n",
    "        heatmap: A 2-D tensor representing the class activation heatmap, \n",
    "                 scaled to [0, 1] with the same spatial dimensions as the input.\n",
    "    \"\"\"\n",
    "    fmap_holder = {}\n",
    "\n",
    "    def save_fmap(_, __, output): \n",
    "        fmap_holder[\"feat\"] = output.detach()\n",
    "\n",
    "    hook = model.layer4[-1].conv3.register_forward_hook(save_fmap)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _ = model(image_tensor) \n",
    "\n",
    "    hook.remove() \n",
    "\n",
    "    feats = fmap_holder[\"feat\"]\n",
    "    weight_vec = model.fc.weight[class_idx]\n",
    "\n",
    "    cam = torch.einsum(\"c,chw->hw\", weight_vec, feats.squeeze(0))\n",
    "    cam = F.relu(cam) \n",
    "    cam = (cam - cam.min()) / (cam.max() + 1e-8)\n",
    "\n",
    "    H, W = image_tensor.shape[2:]\n",
    "    cam_up = F.interpolate( \n",
    "        cam.unsqueeze(0).unsqueeze(0),\n",
    "        size=(H, W),\n",
    "        mode=\"bilinear\", \n",
    "        align_corners=False, \n",
    "    )[0, 0] \n",
    "\n",
    "    heatmap = cam_up.cpu().detach()\n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbfad5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"./fruits_subset/Apple_Rotten/rottenApple_5.jpg\"\n",
    "img = helper_utils.preprocess_image(image_path, device)\n",
    "class_idx = 1\n",
    "\n",
    "heatmap = simplified_cam(model=fruits_model, image_tensor=img, class_idx=class_idx)\n",
    "\n",
    "print(f\"Shape: {heatmap.shape}\")\n",
    "print(f\"Range: min = {heatmap.min()}, max = {heatmap.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43eeca1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"./fruits_subset/Apple_Rotten/rottenApple_5.jpg\"\n",
    "img = helper_utils.preprocess_image(image_path, device)\n",
    "class_idx = 1\n",
    "\n",
    "heatmap = simplified_cam(model=fruits_model, image_tensor=img, class_idx=class_idx)\n",
    "helper_utils.display_cam(img, heatmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9003d92",
   "metadata": {},
   "source": [
    "<a name='5'></a>\n",
    "## 5 - Comparison of Interpretability Techniques\n",
    "\n",
    "| **Aspect** | **Feature Hierarchy** | **Saliency Maps** | **CAM** |\n",
    "|------------|----------------------|-------------------|---------|\n",
    "| **Shows** | Feature evolution across layers | Pixel-level gradient sensitivity | Region-level class evidence |\n",
    "| **Granularity** | Layer-by-layer | Individual pixels | Coarse spatial regions |\n",
    "| **Speed** | Fast | Medium | Fast |\n",
    "| **Requires gradients?** | No | Yes | No |\n",
    "| **Best for** | Model debugging | Adversarial analysis, fine details | Trustworthiness, localization |\n",
    "\n",
    "**Combining techniques:** Use CAM to verify focus on the fruit (not background), Saliency to pinpoint exact pixels driving decisions, and Feature Hierarchy to understand learned representations at each depth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecf6695",
   "metadata": {},
   "source": [
    "<a name='6'></a>\n",
    "## 6 - (Optional) Generative AI for Synthetic Data\n",
    "\n",
    "What happens when you need to train on rare defects but lack enough real photos? This is **data scarcity**—a common bottleneck in industrial AI.\n",
    "\n",
    "Here we flip the script: instead of analyzing how models *interpret* images, we explore how they can *create* new ones using **Stable Diffusion** to synthesize realistic training data from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a541771",
   "metadata": {},
   "source": [
    "<a name='6-1'></a>\n",
    "### 6.1 - Setting up Stable Diffusion\n",
    "\n",
    "We use the Hugging Face `diffusers` library to load a pre-trained Stable Diffusion model, which bundles the text encoder, UNet, and VAE into a single pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cb15cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sd_pipeline(device, model_id=\"stabilityai/stable-diffusion-2-base\"):\n",
    "    \"\"\"\n",
    "    Initializes the Stable Diffusion pipeline from a pretrained model identifier \n",
    "    and transfers it to the specified computing device.\n",
    "\n",
    "    Arguments:\n",
    "        device: The target device (e.g., 'cuda', 'mps', 'cpu') for model execution.\n",
    "        model_id: The repository ID of the pretrained model to load.\n",
    "    \"\"\"\n",
    "    pipe = StableDiffusionPipeline.from_pretrained(\n",
    "        pretrained_model_name_or_path=model_id,\n",
    "        torch_dtype=torch.float16,\n",
    "        variant=\"fp16\",\n",
    "        cache_dir=\"./models\",\n",
    "        local_files_only=True \n",
    "    ).to(device) \n",
    "    \n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79142981",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"pipe\" in globals():\n",
    "    del pipe\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "try:\n",
    "    helper_utils.check_model_snapshot()\n",
    "    pipe = load_sd_pipeline(device)\n",
    "    print(\"\\nLoading Complete!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading pipeline: {e}\")\n",
    "    if \"pipe\" in globals():\n",
    "        del pipe\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    pipe = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db660ad1",
   "metadata": {},
   "source": [
    "<a name='6-2'></a>\n",
    "### 6.2 - Generating Synthetic Data\n",
    "\n",
    "This function takes a text description (e.g., \"A mango with a wormhole\") and generates an image. By controlling the random seed, we ensure reproducibility—important for scientific and industrial contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb16208e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sd_image(pipe, prompt, negative_prompt, seed, steps, save_dir=\"synthetic\"):\n",
    "    \"\"\"\n",
    "    Generates a single image from a text prompt using a pre-loaded Stable Diffusion pipeline.\n",
    "\n",
    "    Arguments:\n",
    "        pipe: The initialized Stable Diffusion pipeline instance.\n",
    "        prompt: The positive text description of the desired image.\n",
    "        negative_prompt: Text description of elements to exclude from the image.\n",
    "        seed: An integer value to initialize the random number generator.\n",
    "        steps: The number of denoising steps to perform during inference.\n",
    "        save_dir: The root directory path where the generated image will be saved.\n",
    "\n",
    "    Returns:\n",
    "        image: The generated PIL Image object.\n",
    "    \"\"\"\n",
    "    device = pipe.device\n",
    "    generator = torch.Generator(device=device).manual_seed(seed)\n",
    "    \n",
    "    image = pipe(\n",
    "        prompt=prompt,\n",
    "        negative_prompt=negative_prompt,\n",
    "        num_inference_steps=steps,\n",
    "        generator=generator,\n",
    "    ).images[0]\n",
    "\n",
    "    slug = \"_\".join(prompt.lower().split()[:3]) \n",
    "    out_dir = Path(save_dir) / slug \n",
    "    out_dir.mkdir(parents=True, exist_ok=True) \n",
    "    out_path = out_dir / f\"img_{seed}.png\" \n",
    "    image.save(out_path)\n",
    "    print(f\"\\nImage saved to {out_path}\\n\")\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c65036",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A mango with a small hole made by a worm in the middle.\"\n",
    "negative_prompt = \"Fresh, intact.\"\n",
    "seed = 42\n",
    "steps = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a20664",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    img = generate_sd_image(\n",
    "        pipe=pipe,\n",
    "        prompt=prompt, \n",
    "        negative_prompt=negative_prompt,\n",
    "        seed=seed, \n",
    "        steps=steps\n",
    "    )\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)\n",
    "except Exception as e:\n",
    "    print(f\"Error during generation: {e}\")\n",
    "    if \"pipe\" in globals():\n",
    "        del pipe\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    pipe = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668dad96",
   "metadata": {},
   "source": [
    "<a name='6-3'></a>\n",
    "### 6.3 - Peeking into the Diffusion Process\n",
    "\n",
    "Stable Diffusion works by iteratively removing noise—starting from random static and gradually nudging pixel values until they match the text prompt. Visualizing this **denoising** process helps us see *when* the model decides on shapes versus when it refines textures.\n",
    "\n",
    "We use a callback mechanism to intercept intermediate latents, decode them into viewable images, and assemble them into a grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f0e788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoising_movie(pipe, prompt, seed, steps, capture_steps, save_grid_path=\"timelapse.png\"):\n",
    "    \"\"\"\n",
    "    Captures intermediate denoising frames from the Stable Diffusion process and \n",
    "    assembles them into a grid image.\n",
    "\n",
    "    Arguments:\n",
    "        pipe: The pre-loaded Stable Diffusion pipeline instance.\n",
    "        prompt: The positive text description for generation.\n",
    "        seed: An integer value for deterministic random noise generation.\n",
    "        steps: The total number of inference steps to perform.\n",
    "        capture_steps: A list of integer indices specifying which steps to capture.\n",
    "        save_grid_path: The file path where the final grid image will be saved.\n",
    "\n",
    "    Returns:\n",
    "        ordered_frames: A list of PIL Image objects corresponding to the captured steps.\n",
    "    \"\"\"\n",
    "    frames = {}\n",
    "\n",
    "    def grab_frame(pipeline, step_idx, timestep, callback_kwargs): \n",
    "        if step_idx in capture_steps:\n",
    "            latents = callback_kwargs[\"latents\"]\n",
    "            with torch.no_grad():\n",
    "                img = pipe.vae.decode(\n",
    "                    latents / pipe.vae.config.scaling_factor,\n",
    "                    return_dict=False\n",
    "                )[0] \n",
    "            pil = pipe.image_processor.postprocess(img, output_type=\"pil\")[0]\n",
    "            frames[step_idx] = pil\n",
    "        return callback_kwargs\n",
    "\n",
    "    generator = torch.Generator(pipe.device).manual_seed(seed)\n",
    "\n",
    "    _ = pipe( \n",
    "        prompt=prompt,\n",
    "        num_inference_steps=steps,\n",
    "        generator=generator,\n",
    "        callback_on_step_end=grab_frame,\n",
    "    ) \n",
    "\n",
    "    ordered_frames = [frames[step] for step in capture_steps]\n",
    "\n",
    "    w, h = ordered_frames[0].size \n",
    "    grid = Image.new(\"RGB\", (w * 2, h * 2)) \n",
    "    for idx, frame in enumerate(ordered_frames): \n",
    "        row, col = divmod(idx, 2) \n",
    "        grid.paste(frame, (col * w, row * h)) \n",
    "\n",
    "    grid.save(save_grid_path) \n",
    "    print(f\"Timelapse grid saved to {save_grid_path}\") \n",
    "\n",
    "    return ordered_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5f4b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A healthy mango.\"\n",
    "seed = 42\n",
    "steps = 50\n",
    "capture_steps = [0, 15, 30, 49]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4a60c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ordered_frames = denoising_movie(\n",
    "        pipe=pipe,\n",
    "        prompt=prompt,\n",
    "        seed=seed, \n",
    "        steps=steps,\n",
    "        capture_steps=capture_steps\n",
    "    )\n",
    "    grid_image = plt.imread(\"timelapse.png\")\n",
    "    plt.axis('off')\n",
    "    plt.imshow(grid_image)\n",
    "except Exception as e:\n",
    "    print(f\"Error during denoising: {e}\")\n",
    "    if \"pipe\" in globals():\n",
    "        del pipe\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    pipe = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35e589d",
   "metadata": {
    "time_run": "2026-02-08T16:12:09.524642+00:00"
   },
   "source": [
    "<a name='7'></a>\n",
    "## 7 - Conclusion\n",
    "\n",
    "So far, we've explored some interpretability techniques to better understand how a deep learning model makes predictions and also generates: \n",
    "\n",
    "- **Feature Hierarchy** revealed how the network builds understanding from edges to complex objects\n",
    "- **Saliency Maps** provided pixel-level sensitivity to verify focus on relevant defects\n",
    "- **Class Activation Maps** showed region-level attention for human-interpretable explanations\n",
    "- **Stable Diffusion** demonstrated how generative AI can address data scarcity by synthesizing realistic training samples\n",
    "\n",
    "These tools form a comprehensive toolkit for explaining, debugging, and improving computer vision systems in real-world applications."
   ]
  }
 ],
 "metadata": {
  "solveit_dialog_mode": "learning",
  "solveit_ver": 2
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
