{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cdc566f",
   "metadata": {
    "time_run": "2026-02-08T04:52:29.316451+00:00"
   },
   "source": [
    "# Classification and Visual Search\n",
    "\n",
    "This notebook builds a dual-purpose architecture for an image-based product catalog system with two capabilities: **classifying** clothing items and powering a **visual search** engine for recommendations.\n",
    "\n",
    "The approach emphasizes **efficiency** and **reusability** — a well-designed feature extractor can be leveraged for multiple downstream tasks.\n",
    "\n",
    "**What we build:**\n",
    "* `InvertedResidualBlock` — the efficient core component inspired by MobileNetV2\n",
    "* `MobileNetBackbone` — stacked custom blocks for feature extraction\n",
    "* `MobileNetLikeClassifier` — multi-class fashion item categorization with weighted loss\n",
    "* `TripleDataset` — generates anchor, positive, and negative examples for similarity learning\n",
    "* `SiameseEncoder` — reuses the trained backbone from the classifier\n",
    "* `SiameseNetwork` — trained with `TripletMarginLoss` for visual similarity\n",
    "* Visual search retrieval using image embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bdca9e",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [Imports](#0)\n",
    "- [1 - Building a Fashion Item Classifier](#1)\n",
    "    - [1.1 - The Fashion Dataset](#1-1)\n",
    "    - [1.2 - Preparing the Data Pipeline](#1-2)\n",
    "    - [1.3 - Architecting the Classifier: Inverted Residuals](#1-3)\n",
    "    - [1.4 - Assembling the Full Classifier](#1-4)\n",
    "    - [1.5 - Training the Classifier](#1-5)  \n",
    "    - [1.6 - Evaluating the Classifier](#1-6)\n",
    "- [2 - Building a Visual Search Engine](#2)\n",
    "    - [2.1 - Similarity Learning for Recommendations](#2-1)\n",
    "    - [2.2 - The Triplet Dataset](#2-2)\n",
    "    - [2.3 - Architecting the Visual Search Model](#2-3)\n",
    "        - [2.3.1 - The Siamese Encoder](#2-3-1)\n",
    "        - [2.3.2 - The Siamese Network Wrapper](#2-3-2)\n",
    "    - [2.4 - Training the Siamese Network](#2-4)\n",
    "    - [2.5 - Performing Visual Search](#2-5)\n",
    "- [3 - Conclusion](#3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a8ddbd",
   "metadata": {
    "time_run": "2026-02-08T04:51:16.597230+00:00"
   },
   "source": [
    "<a name='0'></a>\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3548969d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.utils as vutils\n",
    "from torchvision import transforms\n",
    "import torchinfo\n",
    "import copy\n",
    "\n",
    "import helper_utils\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccac968",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Building a Fashion Item Classifier\n",
    "\n",
    "<a name='1-1'></a>\n",
    "### 1.1 - The Fashion Dataset\n",
    "\n",
    "The dataset is derived from the **clothing-dataset-small** collection, curated into training and validation sets with seven categories: `dress`, `hat`, `longsleeve`, `pants`, `shoes`, `shorts`, and `t-shirt`.\n",
    "\n",
    "<a name='1-2'></a>\n",
    "### 1.2 - Preparing the Data Pipeline\n",
    "\n",
    "The data pipeline defines image transformations for training (with augmentation) and validation (preprocessing only), then loads the datasets and creates DataLoaders for batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512bbdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5] * 3, std=[0.5] * 3),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5] * 3, std=[0.5] * 3),\n",
    "])\n",
    "\n",
    "dataset_path = \"./clothing-dataset-small\"\n",
    "\n",
    "train_dataset, validation_dataset = helper_utils.load_datasets(\n",
    "    dataset_path=dataset_path,\n",
    "    train_transform=train_transform,\n",
    "    val_transform=val_transform,\n",
    ")\n",
    "\n",
    "classes = train_dataset.classes\n",
    "num_classes = len(classes)\n",
    "print(f\"Classes: {classes}\")\n",
    "print(f\"Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98efa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_utils.show_sample_images(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20df250e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader = helper_utils.create_dataloaders(\n",
    "    train_dataset=train_dataset,\n",
    "    validation_dataset=validation_dataset,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0a5cab",
   "metadata": {},
   "source": [
    "<a name='1-3'></a>\n",
    "### 1.3 - Architecting the Classifier: Inverted Residuals\n",
    "\n",
    "The classifier is inspired by **MobileNetV2**, which uses an **Inverted Residual Block** with a narrow → wide → narrow structure. It expands input channels with a 1x1 convolution, applies a lightweight depthwise separable convolution for spatial features, then projects back down with another 1x1 convolution. A skip connection aids gradient flow.\n",
    "\n",
    "The architecture is built modularly:\n",
    "1. `InvertedResidualBlock` — the core efficient block\n",
    "2. `MobileNetBackbone` — stacks blocks for feature extraction\n",
    "3. `MobileNetLikeClassifier` — adds a classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fae4097",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertedResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements an inverted residual block, often used in architectures like MobileNetV2.\n",
    "    \n",
    "    This block features an expansion phase (1x1 convolution), a depthwise\n",
    "    convolution (3x3 convolution), and a projection phase (1x1 convolution).\n",
    "    It utilizes a residual connection between the input and the output of the projection.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, stride, expansion_factor, shortcut=None):\n",
    "        \"\"\"\n",
    "        Initializes the InvertedResidualBlock module.\n",
    "\n",
    "        Args:\n",
    "            in_channels: The number of channels in the input tensor.\n",
    "            out_channels: The number of channels in the output tensor.\n",
    "            stride (int): The stride to be used in the depthwise convolution.\n",
    "            expansion_factor (int): The factor by which to expand the input channels.\n",
    "            shortcut: An optional module for the shortcut connection.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        hidden_dim = in_channels * expansion_factor\n",
    "\n",
    "        self.expand = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, hidden_dim, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.depthwise = nn.Sequential(\n",
    "            nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.project = nn.Sequential(\n",
    "            nn.Conv2d(hidden_dim, out_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "        )\n",
    "\n",
    "        self.shortcut = shortcut\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the InvertedResidualBlock.\n",
    "\n",
    "        Args:\n",
    "            x: The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor after applying the block operations and residual connection.\n",
    "        \"\"\"\n",
    "        skip = x\n",
    "        out = self.expand(x)\n",
    "        out = self.depthwise(out)\n",
    "        out = self.project(out)\n",
    "\n",
    "        if self.shortcut is not None:\n",
    "            skip = self.shortcut(x)\n",
    "\n",
    "        out = out + skip\n",
    "        return F.relu(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bcd8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobileNetBackbone(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements a simplified MobileNet-like backbone feature extractor.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initializes the layers of the MobileNet backbone.\"\"\"\n",
    "        super().__init__()\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.blocks = nn.Sequential(\n",
    "            self._make_block(16, 24, stride=2, expansion_factor=3),\n",
    "            self._make_block(24, 32, stride=2, expansion_factor=3),\n",
    "            self._make_block(32, 64, stride=2, expansion_factor=6),\n",
    "        )\n",
    "\n",
    "    def _make_block(self, in_channels, out_channels, stride=1, expansion_factor=6):\n",
    "        \"\"\"Helper method to create a single InvertedResidualBlock.\"\"\"\n",
    "        if in_channels != out_channels or stride != 1:\n",
    "            shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            )\n",
    "        else:\n",
    "            shortcut = None\n",
    "\n",
    "        return InvertedResidualBlock(in_channels, out_channels, stride, expansion_factor, shortcut)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Defines the forward pass of the backbone.\"\"\"\n",
    "        x = self.stem(x)\n",
    "        x = self.blocks(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d23345",
   "metadata": {},
   "source": [
    "<a name='1-4'></a>\n",
    "### 1.4 - Assembling the Full Classifier\n",
    "\n",
    "The full classifier combines the `MobileNetBackbone` with a classification head that uses Adaptive Average Pooling to reduce spatial dimensions, flattens the result, and applies a Linear layer to produce class logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b48d743",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobileNetLikeClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    A classifier model that combines a feature extraction backbone with a classification head.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=10):\n",
    "        \"\"\"\n",
    "        Initializes the classifier components.\n",
    "\n",
    "        Args:\n",
    "            num_classes (int): The number of output classes.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.backbone = MobileNetBackbone()\n",
    "        self.head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the classifier.\n",
    "\n",
    "        Args:\n",
    "            x: The input tensor (batch of images).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The raw logits for each class.\n",
    "        \"\"\"\n",
    "        x = self.backbone(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "mobilenet_classifier = MobileNetLikeClassifier(num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf102e1",
   "metadata": {},
   "source": [
    "<a name='1-5'></a>\n",
    "### 1.5 - Training the Classifier\n",
    "\n",
    "Training uses weighted `CrossEntropyLoss` to handle class imbalance, the `Adam` optimizer, and a `StepLR` scheduler to reduce learning rate periodically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82121168",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = helper_utils.compute_class_weights(train_dataset).to(device)\n",
    "loss_fcn = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = torch.optim.Adam(mobilenet_classifier.parameters(), lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "n_epochs = 5\n",
    "\n",
    "trained_classifier = helper_utils.training_loop(\n",
    "    mobilenet_classifier, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    loss_fcn, \n",
    "    optimizer, \n",
    "    scheduler, \n",
    "    device, \n",
    "    n_epochs=n_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a68278",
   "metadata": {},
   "source": [
    "<a name='1-6'></a>\n",
    "### 1.6 - Evaluating the Classifier\n",
    "\n",
    "Visualizing predictions from the trained model gives a qualitative sense of performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d1d5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_utils.display_random_predictions_per_class(trained_classifier, val_loader, classes, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bb53ef",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Building a Visual Search Engine\n",
    "\n",
    "<a name='2-1'></a>\n",
    "### 2.1 - Similarity Learning for Recommendations\n",
    "\n",
    "Instead of classifying items, a Siamese Network maps images into an embedding space where visually similar items are located close together. This enables visual search, smarter recommendations, and finding alternatives for out-of-stock items.\n",
    "\n",
    "<a name='2-2'></a>\n",
    "### 2.2 - The Triplet Dataset\n",
    "\n",
    "The network learns from **triplets**: an **anchor** image, a **positive** (same category), and a **negative** (different category). Training pulls anchor-positive embeddings closer while pushing anchor-negative embeddings apart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1558a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripleDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A custom Dataset that returns triplets of images (anchor, positive, negative).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataset):\n",
    "        \"\"\"\n",
    "        Initializes the TripleDataset.\n",
    "\n",
    "        Args:\n",
    "            dataset: The base dataset containing (data, label) pairs.\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.labels = range(len(dataset.classes))\n",
    "        self.labels_to_indices = self._get_labels_to_indices()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def _get_labels_to_indices(self):\n",
    "        \"\"\"Creates a dictionary mapping each label to a list of indices.\"\"\"\n",
    "        labels_to_indices = {}\n",
    "        for idx, (_, label) in enumerate(self.dataset):\n",
    "            if label not in labels_to_indices:\n",
    "                labels_to_indices[label] = []\n",
    "            labels_to_indices[label].append(idx)\n",
    "        return labels_to_indices\n",
    "\n",
    "    def _get_positive_negative_indices(self, anchor_label):\n",
    "        \"\"\"Finds random indices for a positive and a negative sample.\"\"\"\n",
    "        positive_indices = self.labels_to_indices[anchor_label]\n",
    "        positive_index = random.choice(positive_indices)\n",
    "\n",
    "        negative_label = random.choice([label for label in self.labels if label != anchor_label])\n",
    "        negative_indices = self.labels_to_indices[negative_label]\n",
    "        negative_index = random.choice(negative_indices)\n",
    "\n",
    "        return positive_index, negative_index\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Retrieves a triplet (anchor, positive, negative) for a given index.\"\"\"\n",
    "        anchor_image, anchor_label = self.dataset[idx]\n",
    "        positive_index, negative_index = self._get_positive_negative_indices(anchor_label)\n",
    "        positive_image, _ = self.dataset[positive_index]\n",
    "        negative_image, _ = self.dataset[negative_index]\n",
    "\n",
    "        return (anchor_image, positive_image, negative_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94137056",
   "metadata": {},
   "outputs": [],
   "source": [
    "triple_dataset = TripleDataset(train_dataset)\n",
    "\n",
    "siamese_dataloader = torch.utils.data.DataLoader(\n",
    "    triple_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de05d5e4",
   "metadata": {},
   "source": [
    "<a name='2-3'></a>\n",
    "### 2.3 - Architecting the Visual Search Model\n",
    "\n",
    "<a name='2-3-1'></a>\n",
    "#### 2.3.1 - The Siamese Encoder\n",
    "\n",
    "The encoder reuses the `MobileNetBackbone` from the classifier, adding a representation head (pooling + flatten) to produce fixed-size embedding vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9375806",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    An encoder module for Siamese networks that produces fixed-size embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, backbone):\n",
    "        \"\"\"\n",
    "        Initializes the SiameseEncoder.\n",
    "\n",
    "        Args:\n",
    "            backbone (nn.Module): The feature extractor network.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.representation = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Returns the embedding vector for the input.\"\"\"\n",
    "        features = self.backbone(x)\n",
    "        return self.representation(features)\n",
    "\n",
    "siamese_encoder = SiameseEncoder(backbone=trained_classifier.backbone)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29376c17",
   "metadata": {},
   "source": [
    "<a name='2-3-2'></a>\n",
    "#### 2.3.2 - The Siamese Network Wrapper\n",
    "\n",
    "The `SiameseNetwork` passes each triplet image through the shared encoder and returns three embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5333ae4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Siamese Network that processes triplets through a shared embedding network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_network):\n",
    "        \"\"\"\n",
    "        Initializes the SiameseNetwork.\n",
    "\n",
    "        Args:\n",
    "            embedding_network (nn.Module): The shared encoder network.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embedding_network = embedding_network\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        \"\"\"Returns embeddings for anchor, positive, and negative images.\"\"\"\n",
    "        anchor_output = self.embedding_network(anchor)\n",
    "        positive_output = self.embedding_network(positive)\n",
    "        negative_output = self.embedding_network(negative)\n",
    "        return anchor_output, positive_output, negative_output\n",
    "\n",
    "    def get_embedding(self, image):\n",
    "        \"\"\"Generates an embedding for a single image.\"\"\"\n",
    "        return self.embedding_network(image)\n",
    "\n",
    "siamese_network = SiameseNetwork(embedding_network=siamese_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae678bd9",
   "metadata": {},
   "source": [
    "<a name='2-4'></a>\n",
    "### 2.4 - Training the Siamese Network\n",
    "\n",
    "Training uses `TripletMarginLoss` which penalizes the model if the anchor-positive distance isn't smaller than the anchor-negative distance by at least a margin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a832ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fcn = nn.TripletMarginLoss(margin=1.0, p=2.0)\n",
    "optimizer = torch.optim.AdamW(siamese_network.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "helper_utils.siamese_training_loop(\n",
    "    model=siamese_network,\n",
    "    dataloader=siamese_dataloader,\n",
    "    loss_fcn=loss_fcn,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    n_epochs=num_epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6db4ea",
   "metadata": {},
   "source": [
    "<a name='2-5'></a>\n",
    "### 2.5 - Performing Visual Search\n",
    "\n",
    "Visual search works by generating embeddings for a query image and all catalog items, then finding the closest matches by Euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c2abab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_img_embedding(encoder, transform, img, device):\n",
    "    \"\"\"\n",
    "    Generates an embedding vector for a single query PIL image.\n",
    "\n",
    "    Args:\n",
    "        encoder (nn.Module): The trained embedding model.\n",
    "        transform (callable): The torchvision transforms to apply.\n",
    "        img (PIL.Image): The input query image.\n",
    "        device (torch.device): The device to perform inference on.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The embedding vector as a NumPy array.\n",
    "    \"\"\"\n",
    "    tensor_img = transform(img)\n",
    "    query_img_tensor = tensor_img.unsqueeze(0).to(device)\n",
    "    \n",
    "    encoder.eval()\n",
    "    with torch.no_grad():\n",
    "        query_img_embedding = encoder(query_img_tensor)\n",
    "    \n",
    "    return query_img_embedding.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f761c0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = './images/t_shirt.jpg'\n",
    "query_img = helper_utils.get_query_img(image_path)\n",
    "display(query_img)\n",
    "\n",
    "query_img_embedding = get_query_img_embedding(siamese_encoder, val_transform, query_img, device)\n",
    "\n",
    "catalog = validation_dataset\n",
    "embeddings = helper_utils.get_embeddings(siamese_encoder, catalog, device)\n",
    "\n",
    "num_samples = 5\n",
    "closest_indices = helper_utils.find_closest(embeddings, query_img_embedding, num_samples)\n",
    "\n",
    "print(f\"\\nTop {num_samples} similar items:\")\n",
    "for idx_c in closest_indices:\n",
    "    img_c, label_idx_c = helper_utils.get_image(catalog, idx_c)\n",
    "    print(f\"Class: {catalog.classes[label_idx_c]}\")\n",
    "    display(img_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bc2a90",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3 - Conclusion\n",
    "\n",
    "This notebook demonstrated building a dual-purpose AI system: an efficient MobileNet-inspired classifier using `InvertedResidualBlock` and `MobileNetBackbone`, then repurposing that backbone for a Siamese-based visual search engine. Key takeaways include modular architecture design, transfer of learned features between tasks, and the triplet loss approach for learning visual similarity."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "solveit_dialog_mode": "learning",
  "solveit_ver": 2
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
